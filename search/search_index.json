{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"xcube EOPF Data Store","text":"<p><code>xcube-eopf</code> is a Python package that extends xcube with a new data store called <code>\"eopf-zarr\"</code>. This plugin enables the creation of analysis-ready data cubes  (ARDC) from Sentinel products published by the EOPF Sentinel Zarr Sample Service.</p>"},{"location":"#overview","title":"Overview","text":"<p>Once installed, the package gives access to EOPF data products in an analysis-ready  data cube format through the standard xcube data store interface. You can:</p> <ul> <li>List available data sources</li> <li>Check data availability</li> <li>Get metadata of the data sources</li> <li>View available open parameters for each source</li> <li>Open data source directly as xcube Dataset</li> </ul> <p>To explore all available functions, see the Python API.</p> <p>The data retrieval process uses the EOPF STAC API, which allows querying  observations over a specified time range and spatial extent. The resulting datasets  are mosaicked per time step and stacked into a 3D spatiotemporal data cube.</p> <p>Each data variable is returned as a chunked Dask array, supporting efficient  out-of-core computations and visualization.</p> <p>Internally, the package uses the xarray-eopf backend for reading, and leverages  xcube to construct spatiotemporal analysis-ready data cubes.</p>"},{"location":"#features","title":"Features","text":"<p>IMPORTANT <code>xcube-eopf</code> is currently under active development. Some features may be partially implemented or still in progress.</p> <p>The EOPF xcube data store is designed to provide analysis-ready data cubes from the  EOPF Sentinel Zarr samples for Sentinel-1, Sentinel-2, and Sentinel-3 missions. The main features are summarized below. A more in depth documentation is given in the  User Guide. </p> <p>Currently, support is focused on Sentinel-2 and Sentinel-3 products.</p>"},{"location":"#sentinel-1","title":"Sentinel-1","text":"<p>Support for Sentinel-1 will be added in an upcoming release.</p>"},{"location":"#sentinel-2","title":"Sentinel-2","text":"<p>The current implementation supports two Sentinel-2 product levels, available as  <code>data_id</code> values:</p> <ul> <li><code>sentinel-2-l1c</code>: Level-1C top-of-atmosphere reflectance</li> <li><code>sentinel-2-l2a</code>: Level-2A atmospherically corrected surface reflectance</li> </ul>"},{"location":"#cube-generation-workflow","title":"Cube Generation Workflow","text":"<p>The workflow for building 3D analysis-ready cubes from Sentinel-2 products involves  the following steps:</p> <ol> <li>Query products using the EOPF STAC API for a given time range and     spatial extent.</li> <li>Retrieve observations as cloud-optimized Zarr chunks via the     xarray-eopf backend.</li> <li>Mosaic spatial tiles into single images per timestamp.</li> <li>Stack the mosaicked scenes along the temporal axis to form a 3D cube.</li> </ol>"},{"location":"#supported-variables","title":"Supported Variables","text":"<ul> <li>Surface reflectance bands: <code>b01</code>, <code>b02</code>, <code>b03</code>, <code>b04</code>, <code>b05</code>, <code>b06</code>, <code>b07</code>, <code>b08</code>, <code>b8a</code>, <code>b09</code>, <code>b11</code>, <code>b12</code></li> <li>Classification/Quality layers (L2A only): <code>cld</code>, <code>scl</code>, <code>snw</code></li> </ul> <p>Example: Sentinel-2 L2A <pre><code>from xcube.core.store import new_data_store\n\nstore = new_data_store(\"eopf-stac\")\nds = store.open_data(\n    data_id=\"sentinel-2-l2a\",\n    bbox=[9.7, 53.4, 10.3, 53.7],\n    time_range=[\"2025-05-01\", \"2025-05-07\"],\n    spatial_res=10 / 111320,  # meters to degrees (approx.)\n    crs=\"EPSG:4326\",\n    variables=[\"b02\", \"b03\", \"b04\", \"scl\"],\n)\n</code></pre></p>"},{"location":"#sentinel-3","title":"Sentinel-3","text":"<p>The current implementation supports three Sentinel-3 product levels, available as  <code>data_id</code> values:</p> <ul> <li><code>sentinel-3-olci-l1-efr</code>: Level-1 top-of-atmosphere radiance from OLCI instrument</li> <li><code>sentinel-3-olci-l2-lfr</code>: Level-2 land and atmospheric geophysical parameters     derived from OLCI instrument</li> <li><code>sentinel-3-slstr-l1-rbt</code>: Level-1 radiances and brightness temperatures (RBT)    derived from SLSTR instrument</li> <li><code>sentinel-3-slstr-l2-lst</code>: Level-2  land surface temperature derived from SLSTR     instrument</li> </ul>"},{"location":"#cube-generation-workflow_1","title":"Cube Generation Workflow","text":"<p>The workflow for building 3D analysis-ready cubes from Sentinel-3 products involves  the following steps:</p> <ol> <li>Query tiles using the EOPF Zarr Sample Service STAC API for a given time range and     spatial extent.</li> <li>Group items by solar day.</li> <li>Rectify data from the native 2D irregular grid to a regular grid using     xcube-resampling.</li> <li>Mosaic adjacent tiles into seamless daily scenes.</li> <li>Stack the daily mosaics along the temporal axis to form 3D data cubes     for each variable (e.g., spectral bands).</li> </ol>"},{"location":"#supported-variables_1","title":"Supported Variables","text":"<ul> <li><code>sentinel-3-olci-l1-efr</code>: <code>oa01_radiance</code>, <code>oa02_radiance</code>, <code>oa03_radiance</code>, <code>oa04_radiance</code>, <code>oa05_radiance</code>,   <code>oa06_radiance</code>, <code>oa07_radiance</code>, <code>oa08_radiance</code>, <code>oa09_radiance</code>, <code>oa10_radiance</code>,   <code>oa11_radiance</code>, <code>oa12_radiance</code>, <code>oa13_radiance</code>, <code>oa14_radiance</code>, <code>oa15_radiance</code>,   <code>oa16_radiance</code>, <code>oa17_radiance</code>, <code>oa18_radiance</code>, <code>oa19_radiance</code>, <code>oa20_radiance</code>,   <code>oa21_radiance</code></li> <li><code>sentinel-3-olci-l2-lfr</code>: <code>gifapar</code>, <code>iwv</code>, <code>otci</code>, <code>rc681</code>, <code>rc865</code></li> <li><code>sentinel-3-slstr-l1-rbt</code>:   <code>s1_radiance_an</code>, <code>s2_radiance_an</code>, <code>s3_radiance_an</code>, <code>s4_radiance_an</code>,   <code>s5_radiance_an</code>, <code>s6_radiance_an</code>, <code>s1_radiance_ao</code>, <code>s2_radiance_ao</code>,   <code>s3_radiance_ao</code>, <code>s4_radiance_ao</code>, <code>s5_radiance_ao</code>, <code>s6_radiance_ao</code>,   <code>s4_radiance_bn</code>, <code>s5_radiance_bn</code>, <code>s6_radiance_bn</code>, <code>s4_radiance_bo</code>,   <code>s5_radiance_bo</code>, <code>s6_radiance_bo</code>, <code>f1_bt_fn</code>, <code>f1_bt_fo</code>, <code>f2_bt_in</code>,   <code>f2_bt_io</code>, <code>s7_bt_in</code>, <code>s8_bt_in</code>, <code>s9_bt_in</code>, <code>s7_bt_io</code>, <code>s8_bt_io</code>,   <code>s9_bt_io</code></li> <li><code>sentinel-3-slstr-l2-lst</code>:   <code>lst</code></li> </ul> <p>Example: Sentinel-3 SLSTR Level-2 LST <pre><code>from xcube.core.store import new_data_store\n\nstore = new_data_store(\"eopf-stac\")\nds = store.open_data(\n    data_id=\"sentinel-3-slstr-l2-lst\",\n    bbox=[9., 53., 11., 54.],\n    time_range=[\"2025-06-01\", \"2025-06-05\"],\n    spatial_res=300 / 111320, # conversion to degree approx.\n    crs=\"EPSG:4326\"\n)\n</code></pre></p>"},{"location":"#license","title":"License","text":"<p>The package is open source and released under the  Apache 2.0 license license. </p>"},{"location":"about/","title":"About the <code>xcube-eopf</code> project","text":""},{"location":"about/#changelog","title":"Changelog","text":"<p>You can find the complete <code>xcube-eopf</code> changelog  here. </p>"},{"location":"about/#reporting","title":"Reporting","text":"<p>If you have suggestions, ideas, feature requests, or if you have identified a malfunction or error, then please  post an issue. </p>"},{"location":"about/#contributions","title":"Contributions","text":"<p>The <code>xcube-eopf</code> project welcomes contributions of any form as long as you respect our  code of conduct and follow our  contribution guide.</p> <p>If you'd like to submit code or documentation changes, we ask you to provide a  pull request (PR)  here.  For code and configuration changes, your PR must be linked to a  corresponding issue. </p>"},{"location":"about/#development","title":"Development","text":"<p>To install the <code>xcube-eopf</code> development environment into an existing Python  environment, do</p> <pre><code>pip install .[dev,doc]\n</code></pre> <p>or create a new environment using <code>conda</code> or <code>mamba</code></p> <pre><code>mamba env create \n</code></pre>"},{"location":"about/#testing-and-coverage","title":"Testing and Coverage","text":"<p><code>xcube-eopf</code> uses pytest for unit-level testing  and code coverage analysis.</p> <pre><code>pytest tests/ --cov=xarray_eopf --cov-report html\n</code></pre>"},{"location":"about/#some-notes-on-the-strategy-of-unit-testing","title":"Some notes on the strategy of unit-testing","text":"<p>The unit test suite uses pytest-recording to mock STAC catalogs. During development an actual HTTP request is performed to a STAC catalog and the responses are saved in <code>cassettes/**.yaml</code> files. During testing, only the <code>cassettes/**.yaml</code> files are used without an actual HTTP request. During development, to save the responses to <code>cassettes/**.yaml</code>, run</p> <p><pre><code>pytest -v -s --record-mode new_episodes\n</code></pre> Note that <code>--record-mode new_episodes</code> overwrites all cassettes. If the user only wants to write cassettes which are not saved already, <code>--record-mode once</code> can be used. pytest-recording supports all records modes given by VCR.py. After recording the cassettes, testing can be performed as usual.</p>"},{"location":"about/#code-style","title":"Code Style","text":"<p>The <code>xcube-eopf</code> source code is formatted and quality-controlled  using ruff:</p> <pre><code>ruff format\nruff check\n</code></pre>"},{"location":"about/#documentation","title":"Documentation","text":"<p>The <code>xcube-eopf</code> documentation is built using the  mkdocs tool.</p> <p>With repository root as current working directory:</p> <pre><code>pip install .[doc]\n\nmkdocs build\nmkdocs serve\nmkdocs gh-deploy\n</code></pre>"},{"location":"about/#license","title":"License","text":"<p><code>xcube-eopf</code> is open source made available under the terms and conditions of the  Apache 2.0 license.</p>"},{"location":"api/","title":"Python API","text":""},{"location":"api/#xcube.core.store.new_data_store","title":"<code>xcube.core.store.new_data_store(data_store_id, extension_registry=None, **data_store_params)</code>","text":"<p>Create a new data store instance for given data_store_id and data_store_params.</p> <p>Parameters:</p> Name Type Description Default <code>data_store_id</code> <code>str</code> <p>A data store identifier.</p> required <code>extension_registry</code> <code>Optional[ExtensionRegistry]</code> <p>Optional extension registry. If not given, the global extension registry will be used.</p> <code>None</code> <code>**data_store_params</code> <p>Data store specific parameters.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[DataStore, MutableDataStore, PreloadDataStore]</code> <p>A new data store instance</p> Source code in <code>xcube/core/store/store.py</code> <pre><code>def new_data_store(\n    data_store_id: str,\n    extension_registry: Optional[ExtensionRegistry] = None,\n    **data_store_params,\n) -&gt; Union[\"DataStore\", \"MutableDataStore\", \"PreloadDataStore\"]:\n    \"\"\"Create a new data store instance for given\n    *data_store_id* and *data_store_params*.\n\n    Args:\n        data_store_id: A data store identifier.\n        extension_registry: Optional extension registry. If not given,\n            the global extension registry will be used.\n        **data_store_params: Data store specific parameters.\n\n    Returns:\n        A new data store instance\n    \"\"\"\n    data_store_class = get_data_store_class(\n        data_store_id, extension_registry=extension_registry\n    )\n    data_store_params_schema = data_store_class.get_data_store_params_schema()\n    assert_valid_params(\n        data_store_params, name=\"data_store_params\", schema=data_store_params_schema\n    )\n    # noinspection PyArgumentList\n    return data_store_class(**data_store_params)\n</code></pre>"},{"location":"api/#xcube.core.store.list_data_store_ids","title":"<code>xcube.core.store.list_data_store_ids(detail=False)</code>","text":"<p>List the identifiers of installed xcube data stores.</p> <p>Parameters:</p> Name Type Description Default <code>detail</code> <code>bool</code> <p>Whether to return a dictionary with data store metadata or just a list of data store identifiers.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[list[str], dict[str, Any]]</code> <p>If detail is <code>True</code> a dictionary that maps data store identifiers</p> <code>Union[list[str], dict[str, Any]]</code> <p>to data store metadata. Otherwise, a list of data store identifiers.</p> Source code in <code>xcube/core/store/store.py</code> <pre><code>def list_data_store_ids(detail: bool = False) -&gt; Union[list[str], dict[str, Any]]:\n    \"\"\"List the identifiers of installed xcube data stores.\n\n    Args:\n        detail: Whether to return a dictionary with data store metadata or just\n            a list of data store identifiers.\n\n    Returns:\n        If *detail* is ``True`` a dictionary that maps data store identifiers\n        to data store metadata. Otherwise, a list of data store identifiers.\n    \"\"\"\n    if detail:\n        return {e.name: e.metadata for e in find_data_store_extensions()}\n    else:\n        return [e.name for e in find_data_store_extensions()]\n</code></pre>"},{"location":"api/#xcube.core.store.get_data_store_params_schema","title":"<code>xcube.core.store.get_data_store_params_schema(data_store_id, extension_registry=None)</code>","text":"<p>Get the JSON schema for instantiating a new data store identified by data_store_id.</p> <p>Parameters:</p> Name Type Description Default <code>data_store_id</code> <code>str</code> <p>A data store identifier.</p> required <code>extension_registry</code> <code>Optional[ExtensionRegistry]</code> <p>Optional extension registry. If not given, the global extension registry will be used.</p> <code>None</code> <p>Returns:</p> Type Description <code>JsonObjectSchema</code> <p>The JSON schema for the data store's parameters.</p> Source code in <code>xcube/core/store/store.py</code> <pre><code>def get_data_store_params_schema(\n    data_store_id: str, extension_registry: Optional[ExtensionRegistry] = None\n) -&gt; JsonObjectSchema:\n    \"\"\"Get the JSON schema for instantiating a new data store\n    identified by *data_store_id*.\n\n    Args:\n        data_store_id: A data store identifier.\n        extension_registry: Optional extension registry. If not given,\n            the global extension registry will be used.\n\n    Returns:\n        The JSON schema for the data store's parameters.\n    \"\"\"\n    data_store_class = get_data_store_class(\n        data_store_id, extension_registry=extension_registry\n    )\n    return data_store_class.get_data_store_params_schema()\n</code></pre>"},{"location":"api/#xcube_eopf.store.EOPFZarrDataStore","title":"<code>xcube_eopf.store.EOPFZarrDataStore</code>","text":"<p>               Bases: <code>DataStore</code></p> <p>EOPF-Zarr implementation of the data store.</p> Source code in <code>xcube_eopf/store.py</code> <pre><code>class EOPFZarrDataStore(DataStore):\n    \"\"\"EOPF-Zarr implementation of the data store.\"\"\"\n\n    def __init__(self):\n        register_product_handlers()\n\n    @classmethod\n    def get_data_store_params_schema(cls) -&gt; JsonObjectSchema:\n        return JsonObjectSchema(\n            description=\"Describes the parameters of the xcube data store 'eopf-zarr'.\",\n            properties=dict(),\n            required=[],\n            additional_properties=False,\n        )\n\n    @classmethod\n    def get_data_types(cls) -&gt; tuple[str, ...]:\n        return (DATASET_TYPE.alias,)\n\n    def get_data_types_for_data(self, data_id: str) -&gt; tuple[str, ...]:\n        self._assert_has_data(data_id)\n        return (DATASET_TYPE.alias,)\n\n    def get_data_ids(\n        self,\n        data_type: DataTypeLike = None,\n        include_attrs: Container[str] | bool = False,\n    ) -&gt; Iterator[str | tuple[str, dict[str, Any]], None]:\n        self._assert_valid_data_type(data_type)\n        for data_id in ProductHandler.registry.keys():\n            if not include_attrs:\n                yield data_id\n            else:\n                yield data_id, dict()\n\n    def has_data(self, data_id: str, data_type: DataTypeLike = None) -&gt; bool:\n        self._assert_valid_data_type(data_type)\n        if data_id in ProductHandler.registry.keys():\n            return True\n        return False\n\n    def get_data_opener_ids(\n        self, data_id: str = None, data_type: DataTypeLike = None\n    ) -&gt; tuple[str, ...]:\n        self._assert_valid_data_type(data_type)\n        if data_id is not None:\n            self._assert_has_data(data_id)\n        return (EOPF_ZARR_OPENR_ID,)\n\n    def get_open_data_params_schema(\n        self, data_id: str = None, opener_id: str = None\n    ) -&gt; JsonObjectSchema:\n        self._assert_valid_opener_id(opener_id)\n        if data_id is not None:\n            self._assert_has_data(data_id)\n            product_handler = ProductHandler.guess(data_id)\n            return product_handler.get_open_data_params_schema()\n        else:\n            return JsonObjectSchema(\n                title=\"Opening parameters for all supported Sentinel products.\",\n                properties={\n                    key: ph.get_open_data_params_schema()\n                    for (key, ph) in zip(\n                        ProductHandler.registry.keys(), ProductHandler.registry.values()\n                    )\n                },\n            )\n\n    def open_data(\n        self,\n        data_id: str,\n        opener_id: str = None,\n        data_type: DataTypeLike = None,\n        **open_params,\n    ) -&gt; xr.Dataset | MultiLevelDataset:\n        self._assert_has_data(data_id)\n        self._assert_valid_data_type(data_type)\n        self._assert_valid_opener_id(opener_id)\n        schema = self.get_open_data_params_schema(data_id=data_id)\n        schema.validate_instance(open_params)\n\n        product_handler = ProductHandler.guess(data_id)\n\n        # search for items\n        search_params = product_handler.prepare_stac_queries(data_id, open_params)\n        catalog = pystac_client.Client.open(STAC_URL)\n        items = list(catalog.search(**search_params).items())\n        # filter deprecated items\n        items = filter_items_deprecated(items)\n        # fiter items with incorrectly assigned footprint\n        items = filter_items_wrong_footprint(items)\n        if len(items) == 0:\n            raise DataStoreError(f\"No items found for search_params {search_params}.\")\n\n        return product_handler.open_data(items, **open_params)\n\n    def describe_data(\n        self, data_id: str, data_type: DataTypeLike = None\n    ) -&gt; DatasetDescriptor:\n        self._assert_has_data(data_id)\n        self._assert_valid_data_type(data_type)\n        raise NotImplementedError(\"`describe_data` is not implemented, yet.\")\n\n    def search_data(\n        self, data_type: DataTypeLike = None, **search_params\n    ) -&gt; Iterator[DatasetDescriptor]:\n        raise NotImplementedError(\n            \"Search is not supported. Only Sentinel-2 L1C and L2A products \"\n            \"are currently handled.\"\n        )\n\n    def get_search_params_schema(\n        self, data_type: DataTypeLike = None\n    ) -&gt; JsonObjectSchema:\n        self._assert_valid_data_type(data_type)\n        return JsonObjectSchema(\n            properties=dict(),\n            required=[],\n            additional_properties=True,\n        )\n\n    # Auxiliary internal functions\n    def _assert_has_data(self, data_id: str) -&gt; None:\n        if not self.has_data(data_id):\n            raise DataStoreError(f\"Data resource {data_id!r} is not available.\")\n\n    @staticmethod\n    def _assert_valid_opener_id(opener_id: str) -&gt; None:\n        if opener_id is not None and opener_id is not EOPF_ZARR_OPENR_ID:\n            raise DataStoreError(\n                f\"Data opener identifier must be {EOPF_ZARR_OPENR_ID!r}, \"\n                f\"but got {opener_id!r}.\"\n            )\n\n    def _assert_valid_data_type(self, data_type: DataTypeLike) -&gt; None:\n        if not self._is_valid_data_type(data_type):\n            raise DataStoreError(\n                f\"Data type must be {DATASET_TYPE.alias!r} \"\n                f\"or None, but got {data_type!r}.\"\n            )\n\n    @staticmethod\n    def _is_valid_data_type(data_type: DataTypeLike) -&gt; bool:\n        return data_type is None or DATASET_TYPE.is_super_type_of(data_type)\n</code></pre>"},{"location":"api/#xcube.core.store.DataStore","title":"<code>xcube.core.store.DataStore</code>","text":"<p>               Bases: <code>DataOpener</code>, <code>DataSearcher</code>, <code>DataPreloader</code>, <code>ABC</code></p> <p>A data store represents a collection of data resources that can be enumerated, queried, and opened in order to obtain in-memory representations of the data. The same data resource may be made available using different data types. Therefore, many methods allow specifying a data_type parameter.</p> <p>A store implementation may use any existing openers/writers, or define its own, or not use any openers/writers at all.</p> <p>Store implementers should follow the conventions outlined in https://xcube.readthedocs.io/en/latest/storeconv.html .</p> <p>The :class:<code>DataStore</code> is an abstract base class that both read-only and mutable data stores must implement.</p> Source code in <code>xcube/core/store/store.py</code> <pre><code>class DataStore(DataOpener, DataSearcher, DataPreloader, ABC):\n    \"\"\"A data store represents a collection of data resources that\n    can be enumerated, queried, and opened in order to obtain\n    in-memory representations of the data. The same data resource may be\n    made available using different data types. Therefore, many methods\n    allow specifying a *data_type* parameter.\n\n    A store implementation may use any existing openers/writers,\n    or define its own, or not use any openers/writers at all.\n\n    Store implementers should follow the conventions outlined in\n    https://xcube.readthedocs.io/en/latest/storeconv.html .\n\n    The :class:`DataStore` is an abstract base class that both read-only and\n    mutable data stores must implement.\n    \"\"\"\n\n    @classmethod\n    def get_data_store_params_schema(cls) -&gt; JsonObjectSchema:\n        \"\"\"Get descriptions of parameters that must or can be used to\n        instantiate a new DataStore object.\n        Parameters are named and described by the properties of the\n        returned JSON object schema.\n        The default implementation returns JSON object schema that\n        can have any properties.\n        \"\"\"\n        return JsonObjectSchema()\n\n    @classmethod\n    @abstractmethod\n    def get_data_types(cls) -&gt; tuple[str, ...]:\n        \"\"\"Get alias names for all data types supported by this store.\n        The first entry in the tuple represents this store's\n        default data type.\n\n        Returns:\n            The tuple of supported data types.\n        \"\"\"\n\n    @abstractmethod\n    def get_data_types_for_data(self, data_id: str) -&gt; tuple[str, ...]:\n        \"\"\"Get alias names for of data types that are supported\n        by this store for the given *data_id*.\n\n        Args:\n            data_id: An identifier of data that is provided by this\n                store\n\n        Returns:\n            A tuple of data types that apply to the given *data_id*.\n\n        Raises:\n            DataStoreError: If an error occurs.\n        \"\"\"\n\n    @abstractmethod\n    def get_data_ids(\n        self,\n        data_type: DataTypeLike = None,\n        include_attrs: Container[str] | bool = False,\n    ) -&gt; Union[Iterator[str], Iterator[tuple[str, dict[str, Any]]]]:\n        \"\"\"Get an iterator over the data resource identifiers for the\n        given type *data_type*. If *data_type* is omitted, all data\n        resource identifiers are returned.\n\n        If a store implementation supports only a single data type,\n        it should verify that *data_type* is either None or\n        compatible with the supported data type.\n\n        If *include_attrs* is provided, it must be a sequence of names\n        of metadata attributes. The store will then return extra metadata\n        for each returned data resource identifier according to the\n        names of the metadata attributes as tuples (*data_id*, *attrs*).\n\n        Hence, the type of the returned iterator items depends on the\n        value of *include_attrs*:\n\n        - If *include_attrs* is False (the default), the method returns\n          an iterator of dataset identifiers *data_id* of type `str`.\n        - If *include_attrs* is True, the method returns an iterator of tuples\n          (*data_id*, *attrs*) of type `Tuple[str, Dict]`, where *attrs*\n          is a dictionary filled with all the attributes available respectively\n          for each *data_id*.\n        - If *include_attrs* is a sequence of attribute names, even an\n          empty one, the method returns an iterator of tuples\n          (*data_id*, *attrs*) of type `Tuple[str, Dict]`, where *attrs*\n          is a dictionary filled according to the names in *include_attrs*.\n          If a store cannot provide a given attribute, it should simply\n          ignore it. This may even yield to an empty dictionary for a given\n          *data_id*.\n\n        The individual attributes do not have to exist in the dataset's\n        metadata, they may also be generated on-the-fly.\n        An example for a generic attribute name is \"title\".\n        A store should try to resolve ``include_attrs=[\"title\"]``\n        by returning items such as\n        ``(\"ESACCI-L4_GHRSST-SSTdepth-OSTIA-GLOB_CDR2.1-v02.0-fv01.0.zarr\",\n        {\"title\": \"Level-4 GHRSST Analysed Sea Surface Temperature\"})``.\n\n        Args:\n            data_type: If given, only data identifiers that are\n                available as this type are returned. If this is omitted,\n                all available data identifiers are returned.\n            include_attrs: A sequence of names of attributes to be\n                returned for each dataset identifier. If given, the\n                store will attempt to provide the set of requested\n                dataset attributes in addition to the data ids. (added\n                in xcube 0.8.0)\n\n        Returns:\n            An iterator over the identifiers and titles of data\n            resources provided by this data store.\n\n        Raises:\n            DataStoreError: If an error occurs.\n        \"\"\"\n\n    def list_data_ids(\n        self,\n        data_type: DataTypeLike = None,\n        include_attrs: Container[str] | bool = False,\n    ) -&gt; Union[list[str], list[tuple[str, dict[str, Any]]]]:\n        \"\"\"Convenience version of `get_data_ids()` that returns a list rather\n        than an iterator.\n\n        Args:\n            data_type: If given, only data identifiers that are\n                available as this type are returned. If this is omitted,\n                all available data identifiers are returned.\n            include_attrs: A boolean or sequence of names of attributes to be\n                returned for each dataset identifier. If a sequence of names of\n                attributes given, the store will attempt to provide the set of requested\n                dataset attributes in addition to the data ids. (added\n                in xcube 0.8.0).\n                If True, all the attributes for each dataset identifier will be\n                returned.\n                If False (default), only the data_ids are returned.\n\n        Returns:\n            A list comprising the identifiers and titles of data\n            resources provided by this data store.\n\n        Raises:\n            DataStoreError: If an error occurs.\n        \"\"\"\n        return list(self.get_data_ids(data_type=data_type, include_attrs=include_attrs))\n\n    @abstractmethod\n    def has_data(self, data_id: str, data_type: DataTypeLike = None) -&gt; bool:\n        \"\"\"Check if the data resource given by *data_id* is\n        available in this store.\n\n        Args:\n            data_id: A data identifier\n            data_type: An optional data type. If given, it will also be\n                checked whether the data is available as the specified\n                type. May be given as type alias name, as a type, or as\n                a :class:`DataType` instance.\n\n        Returns:\n            True, if the data resource is available in this store, False\n            otherwise.\n        \"\"\"\n\n    @abstractmethod\n    def describe_data(\n        self, data_id: str, data_type: DataTypeLike = None\n    ) -&gt; DataDescriptor:\n        \"\"\"Get the descriptor for the data resource given by *data_id*.\n\n        Raises a :class:`DataStoreError` if *data_id* does not\n        exist in this store or the data is not available as the\n        specified *data_type*.\n\n        Args:\n            data_id: An identifier of data provided by this store\n            data_type: If given, the descriptor of the data will\n                describe the data as specified by the data type. May be\n                given as type alias name, as a type, or as a\n                :class:`DataType` instance.\n\n        Returns: a data-type specific data descriptor\n\n        Raises:\n            DataStoreError: If an error occurs.\n        \"\"\"\n\n    @abstractmethod\n    def get_data_opener_ids(\n        self, data_id: str = None, data_type: DataTypeLike = None\n    ) -&gt; tuple[str, ...]:\n        \"\"\"Get identifiers of data openers that can be used to open data\n        resources from this store.\n\n        If *data_id* is given, data accessors are restricted to the ones\n        that can open the identified data resource.\n        Raises if *data_id* does not exist in this store.\n\n        If *data_type* is given, only openers that are compatible with\n        this data type are returned.\n\n        If a store implementation supports only a single data type,\n        it should verify that *data_type* is either None or equal to\n        that single data type.\n\n        Args:\n            data_id: An optional data resource identifier that is known\n                to exist in this data store.\n            data_type: An optional data type that is known to be\n                supported by this data store. May be given as type alias\n                name, as a type, or as a :class:`DataType` instance.\n\n        Returns:\n            A tuple of identifiers of data openers that can be used to\n            open data resources.\n\n        Raises:\n            DataStoreError: If an error occurs.\n        \"\"\"\n\n    @abstractmethod\n    def get_open_data_params_schema(\n        self, data_id: str = None, opener_id: str = None\n    ) -&gt; JsonObjectSchema:\n        \"\"\"Get the schema for the parameters passed as *open_params* to\n        :meth:`open_data`.\n\n        If *data_id* is given, the returned schema will be tailored\n        to the constraints implied by the identified data resource.\n        Some openers might not support this, therefore *data_id* is optional,\n        and if it is omitted, the returned schema will be less restrictive.\n        If given, the method raises if *data_id* does not exist in this store.\n\n        If *opener_id* is given, the returned schema will be tailored to\n        the constraints implied by the identified opener. Some openers\n        might not support this, therefore *opener_id* is optional, and if\n        it is omitted, the returned schema will be less restrictive.\n\n        For maximum compatibility of stores, it is strongly encouraged to\n        apply the following conventions on parameter names, types,\n        and their interpretation.\n\n        Let P be the value of an optional, data constraining open parameter,\n        then it should be interpreted as follows:\n\n          * _if P is None_ means, parameter not given,\n            hence no constraint applies, hence no additional restrictions\n            on requested data.\n          * _if not P_ means, we exclude data that would be\n            included by default.\n          * _else_, the given constraint applies.\n\n        Given here are names, types, and descriptions of common,\n        constraining open parameters for gridded datasets.\n        Note, whether any of these is optional or mandatory depends\n        on the individual data store. A store may also\n        define other open parameters or support only a subset of the\n        following. Note all parameters may be optional,\n        the Python-types given here refer to _given_, non-Null parameters:\n\n          * ``variable_names: List[str]``: Included data variables.\n            Available coordinate variables will be auto-included for\n            any dimension of the data variables.\n          * ``bbox: Tuple[float, float, float, float]``: Spatial coverage\n            as xmin, ymin, xmax, ymax.\n          * ``crs: str``: Spatial CRS, e.g. \"EPSG:4326\" or OGC CRS URI.\n          * ``spatial_res: float``: Spatial resolution in\n            coordinates of the spatial CRS.\n          * ``time_range: Tuple[Optional[str], Optional[str]]``:\n            Time range interval in UTC date/time units using ISO format.\n            Start or end time may be missing which means everything until\n            available start or end time.\n          * ``time_period: str`: Pandas-compatible period/frequency\n            string, e.g. \"8D\", \"2W\".\n\n        E.g. applied to an optional `variable_names` parameter, this means\n\n          * `variable_names is None` - include all data variables\n          * `variable_names == []` - do not include data variables\n            (schema only)\n          * `variable_names == [\"&lt;var_1&gt;\", \"&lt;var_2&gt;\", ...]` only\n            include data variables named \"&lt;var_1&gt;\", \"&lt;var_2&gt;\", ...\n\n        Args:\n            data_id: An optional data identifier that is known to exist\n                in this data store.\n            opener_id: An optional data opener identifier.\n\n        Returns:\n            The schema for the parameters in *open_params*.\n\n        Raises:\n            DataStoreError: If an error occurs.\n        \"\"\"\n\n    @abstractmethod\n    def open_data(\n        self,\n        data_id: str,\n        opener_id: str = None,\n        **open_params,\n    ) -&gt; Any:\n        \"\"\"Open the data given by the data resource identifier *data_id*\n        using the supplied *open_params*.\n\n        If *opener_id* is given, the identified data opener will be used\n        to open the data resource and *open_params* must comply with the\n        schema of the opener's parameters. Note that some store\n        implementations may not support using different openers or just\n        support a single one.\n\n        Implementations are advised to support an additional optional keyword\n        argument `data_type: DataTypeLike = None`.\n        If *data_type* is provided, the return value will be in the specified\n        data type. If no data opener exists for the given *data_type* and format\n        extracted from the *data_id*, the default data type alias 'dataset' will\n        be used. Note that *opener_id* includes the *data_type* at its first\n        position and will override the *date_type* argument.\n\n        Raises if *data_id* does not exist in this store.\n\n        Args:\n            data_id: The data identifier that is known to exist in this\n                data store.\n            opener_id: An optional data opener identifier.\n            **open_params: Opener-specific parameters. Note that\n                `data_type: DataTypeLike = None` may be assigned here.\n\n        Returns:\n            An in-memory representation of the data resources identified\n            by *data_id* and *open_params*.\n\n        Raises:\n            DataStoreError: If an error occurs.\n        \"\"\"\n\n    def get_preload_data_params_schema(self) -&gt; JsonObjectSchema:\n        \"\"\"Get the JSON schema that describes the keyword\n        arguments that can be passed to ``preload_data()``.\n\n        Returns:\n            A ``JsonObjectSchema`` object whose properties describe\n            the parameters of ``preload_data()``.\n        \"\"\"\n        return JsonObjectSchema(additional_properties=False)\n\n    # noinspection PyMethodMayBeStatic\n    def preload_data(\n        self,\n        *data_ids: str,\n        **preload_params: Any,\n    ) -&gt; \"PreloadedDataStore\":\n        \"\"\"Preload the given data items for faster access.\n\n        Warning: This is an experimental and potentially unstable API\n        introduced in xcube 1.8.\n\n        The method may be blocking or non-blocking.\n        Implementations may offer the following keyword arguments\n        in *preload_params*:\n\n        - ``blocking``: whether the preload process is blocking.\n          Should be `True` by default if supported.\n        - ``monitor``: a callback function that serves as a progress monitor.\n          It receives the preload handle and the recent partial state update.\n\n        Args:\n            data_ids: Data identifiers to be preloaded.\n            preload_params: data store specific preload parameters.\n              See method ``get_preload_data_params_schema()`` for information\n              on the possible options.\n\n        Returns:\n            A mutable data store containing the preload handle.\n            The default implementation contains an empty preload handle.\n        \"\"\"\n        self.preload_handle = NullPreloadHandle()\n        return self\n</code></pre>"},{"location":"api/#xcube.core.store.DataStore.get_data_ids","title":"<code>get_data_ids(data_type=None, include_attrs=False)</code>  <code>abstractmethod</code>","text":"<p>Get an iterator over the data resource identifiers for the given type data_type. If data_type is omitted, all data resource identifiers are returned.</p> <p>If a store implementation supports only a single data type, it should verify that data_type is either None or compatible with the supported data type.</p> <p>If include_attrs is provided, it must be a sequence of names of metadata attributes. The store will then return extra metadata for each returned data resource identifier according to the names of the metadata attributes as tuples (data_id, attrs).</p> <p>Hence, the type of the returned iterator items depends on the value of include_attrs:</p> <ul> <li>If include_attrs is False (the default), the method returns   an iterator of dataset identifiers data_id of type <code>str</code>.</li> <li>If include_attrs is True, the method returns an iterator of tuples   (data_id, attrs) of type <code>Tuple[str, Dict]</code>, where attrs   is a dictionary filled with all the attributes available respectively   for each data_id.</li> <li>If include_attrs is a sequence of attribute names, even an   empty one, the method returns an iterator of tuples   (data_id, attrs) of type <code>Tuple[str, Dict]</code>, where attrs   is a dictionary filled according to the names in include_attrs.   If a store cannot provide a given attribute, it should simply   ignore it. This may even yield to an empty dictionary for a given   data_id.</li> </ul> <p>The individual attributes do not have to exist in the dataset's metadata, they may also be generated on-the-fly. An example for a generic attribute name is \"title\". A store should try to resolve <code>include_attrs=[\"title\"]</code> by returning items such as <code>(\"ESACCI-L4_GHRSST-SSTdepth-OSTIA-GLOB_CDR2.1-v02.0-fv01.0.zarr\", {\"title\": \"Level-4 GHRSST Analysed Sea Surface Temperature\"})</code>.</p> <p>Parameters:</p> Name Type Description Default <code>data_type</code> <code>DataTypeLike</code> <p>If given, only data identifiers that are available as this type are returned. If this is omitted, all available data identifiers are returned.</p> <code>None</code> <code>include_attrs</code> <code>Container[str] | bool</code> <p>A sequence of names of attributes to be returned for each dataset identifier. If given, the store will attempt to provide the set of requested dataset attributes in addition to the data ids. (added in xcube 0.8.0)</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[Iterator[str], Iterator[tuple[str, dict[str, Any]]]]</code> <p>An iterator over the identifiers and titles of data</p> <code>Union[Iterator[str], Iterator[tuple[str, dict[str, Any]]]]</code> <p>resources provided by this data store.</p> <p>Raises:</p> Type Description <code>DataStoreError</code> <p>If an error occurs.</p> Source code in <code>xcube/core/store/store.py</code> <pre><code>@abstractmethod\ndef get_data_ids(\n    self,\n    data_type: DataTypeLike = None,\n    include_attrs: Container[str] | bool = False,\n) -&gt; Union[Iterator[str], Iterator[tuple[str, dict[str, Any]]]]:\n    \"\"\"Get an iterator over the data resource identifiers for the\n    given type *data_type*. If *data_type* is omitted, all data\n    resource identifiers are returned.\n\n    If a store implementation supports only a single data type,\n    it should verify that *data_type* is either None or\n    compatible with the supported data type.\n\n    If *include_attrs* is provided, it must be a sequence of names\n    of metadata attributes. The store will then return extra metadata\n    for each returned data resource identifier according to the\n    names of the metadata attributes as tuples (*data_id*, *attrs*).\n\n    Hence, the type of the returned iterator items depends on the\n    value of *include_attrs*:\n\n    - If *include_attrs* is False (the default), the method returns\n      an iterator of dataset identifiers *data_id* of type `str`.\n    - If *include_attrs* is True, the method returns an iterator of tuples\n      (*data_id*, *attrs*) of type `Tuple[str, Dict]`, where *attrs*\n      is a dictionary filled with all the attributes available respectively\n      for each *data_id*.\n    - If *include_attrs* is a sequence of attribute names, even an\n      empty one, the method returns an iterator of tuples\n      (*data_id*, *attrs*) of type `Tuple[str, Dict]`, where *attrs*\n      is a dictionary filled according to the names in *include_attrs*.\n      If a store cannot provide a given attribute, it should simply\n      ignore it. This may even yield to an empty dictionary for a given\n      *data_id*.\n\n    The individual attributes do not have to exist in the dataset's\n    metadata, they may also be generated on-the-fly.\n    An example for a generic attribute name is \"title\".\n    A store should try to resolve ``include_attrs=[\"title\"]``\n    by returning items such as\n    ``(\"ESACCI-L4_GHRSST-SSTdepth-OSTIA-GLOB_CDR2.1-v02.0-fv01.0.zarr\",\n    {\"title\": \"Level-4 GHRSST Analysed Sea Surface Temperature\"})``.\n\n    Args:\n        data_type: If given, only data identifiers that are\n            available as this type are returned. If this is omitted,\n            all available data identifiers are returned.\n        include_attrs: A sequence of names of attributes to be\n            returned for each dataset identifier. If given, the\n            store will attempt to provide the set of requested\n            dataset attributes in addition to the data ids. (added\n            in xcube 0.8.0)\n\n    Returns:\n        An iterator over the identifiers and titles of data\n        resources provided by this data store.\n\n    Raises:\n        DataStoreError: If an error occurs.\n    \"\"\"\n</code></pre>"},{"location":"api/#xcube.core.store.DataStore.list_data_ids","title":"<code>list_data_ids(data_type=None, include_attrs=False)</code>","text":"<p>Convenience version of <code>get_data_ids()</code> that returns a list rather than an iterator.</p> <p>Parameters:</p> Name Type Description Default <code>data_type</code> <code>DataTypeLike</code> <p>If given, only data identifiers that are available as this type are returned. If this is omitted, all available data identifiers are returned.</p> <code>None</code> <code>include_attrs</code> <code>Container[str] | bool</code> <p>A boolean or sequence of names of attributes to be returned for each dataset identifier. If a sequence of names of attributes given, the store will attempt to provide the set of requested dataset attributes in addition to the data ids. (added in xcube 0.8.0). If True, all the attributes for each dataset identifier will be returned. If False (default), only the data_ids are returned.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[list[str], list[tuple[str, dict[str, Any]]]]</code> <p>A list comprising the identifiers and titles of data</p> <code>Union[list[str], list[tuple[str, dict[str, Any]]]]</code> <p>resources provided by this data store.</p> <p>Raises:</p> Type Description <code>DataStoreError</code> <p>If an error occurs.</p> Source code in <code>xcube/core/store/store.py</code> <pre><code>def list_data_ids(\n    self,\n    data_type: DataTypeLike = None,\n    include_attrs: Container[str] | bool = False,\n) -&gt; Union[list[str], list[tuple[str, dict[str, Any]]]]:\n    \"\"\"Convenience version of `get_data_ids()` that returns a list rather\n    than an iterator.\n\n    Args:\n        data_type: If given, only data identifiers that are\n            available as this type are returned. If this is omitted,\n            all available data identifiers are returned.\n        include_attrs: A boolean or sequence of names of attributes to be\n            returned for each dataset identifier. If a sequence of names of\n            attributes given, the store will attempt to provide the set of requested\n            dataset attributes in addition to the data ids. (added\n            in xcube 0.8.0).\n            If True, all the attributes for each dataset identifier will be\n            returned.\n            If False (default), only the data_ids are returned.\n\n    Returns:\n        A list comprising the identifiers and titles of data\n        resources provided by this data store.\n\n    Raises:\n        DataStoreError: If an error occurs.\n    \"\"\"\n    return list(self.get_data_ids(data_type=data_type, include_attrs=include_attrs))\n</code></pre>"},{"location":"api/#xcube.core.store.DataStore.has_data","title":"<code>has_data(data_id, data_type=None)</code>  <code>abstractmethod</code>","text":"<p>Check if the data resource given by data_id is available in this store.</p> <p>Parameters:</p> Name Type Description Default <code>data_id</code> <code>str</code> <p>A data identifier</p> required <code>data_type</code> <code>DataTypeLike</code> <p>An optional data type. If given, it will also be checked whether the data is available as the specified type. May be given as type alias name, as a type, or as a :class:<code>DataType</code> instance.</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True, if the data resource is available in this store, False</p> <code>bool</code> <p>otherwise.</p> Source code in <code>xcube/core/store/store.py</code> <pre><code>@abstractmethod\ndef has_data(self, data_id: str, data_type: DataTypeLike = None) -&gt; bool:\n    \"\"\"Check if the data resource given by *data_id* is\n    available in this store.\n\n    Args:\n        data_id: A data identifier\n        data_type: An optional data type. If given, it will also be\n            checked whether the data is available as the specified\n            type. May be given as type alias name, as a type, or as\n            a :class:`DataType` instance.\n\n    Returns:\n        True, if the data resource is available in this store, False\n        otherwise.\n    \"\"\"\n</code></pre>"},{"location":"api/#xcube.core.store.DataStore.get_open_data_params_schema","title":"<code>get_open_data_params_schema(data_id=None, opener_id=None)</code>  <code>abstractmethod</code>","text":"<p>Get the schema for the parameters passed as open_params to :meth:<code>open_data</code>.</p> <p>If data_id is given, the returned schema will be tailored to the constraints implied by the identified data resource. Some openers might not support this, therefore data_id is optional, and if it is omitted, the returned schema will be less restrictive. If given, the method raises if data_id does not exist in this store.</p> <p>If opener_id is given, the returned schema will be tailored to the constraints implied by the identified opener. Some openers might not support this, therefore opener_id is optional, and if it is omitted, the returned schema will be less restrictive.</p> <p>For maximum compatibility of stores, it is strongly encouraged to apply the following conventions on parameter names, types, and their interpretation.</p> <p>Let P be the value of an optional, data constraining open parameter, then it should be interpreted as follows:</p> <ul> <li>if P is None means, parameter not given,     hence no constraint applies, hence no additional restrictions     on requested data.</li> <li>if not P means, we exclude data that would be     included by default.</li> <li>else, the given constraint applies.</li> </ul> <p>Given here are names, types, and descriptions of common, constraining open parameters for gridded datasets. Note, whether any of these is optional or mandatory depends on the individual data store. A store may also define other open parameters or support only a subset of the following. Note all parameters may be optional, the Python-types given here refer to given, non-Null parameters:</p> <ul> <li><code>variable_names: List[str]</code>: Included data variables.     Available coordinate variables will be auto-included for     any dimension of the data variables.</li> <li><code>bbox: Tuple[float, float, float, float]</code>: Spatial coverage     as xmin, ymin, xmax, ymax.</li> <li><code>crs: str</code>: Spatial CRS, e.g. \"EPSG:4326\" or OGC CRS URI.</li> <li><code>spatial_res: float</code>: Spatial resolution in     coordinates of the spatial CRS.</li> <li><code>time_range: Tuple[Optional[str], Optional[str]]</code>:     Time range interval in UTC date/time units using ISO format.     Start or end time may be missing which means everything until     available start or end time.</li> <li><code>`time_period: str</code>: Pandas-compatible period/frequency     string, e.g. \"8D\", \"2W\".</li> </ul> <p>E.g. applied to an optional <code>variable_names</code> parameter, this means</p> <ul> <li><code>variable_names is None</code> - include all data variables</li> <li><code>variable_names == []</code> - do not include data variables     (schema only)</li> <li><code>variable_names == [\"&lt;var_1&gt;\", \"&lt;var_2&gt;\", ...]</code> only     include data variables named \"\", \"\", ... <p>Parameters:</p> Name Type Description Default <code>data_id</code> <code>str</code> <p>An optional data identifier that is known to exist in this data store.</p> <code>None</code> <code>opener_id</code> <code>str</code> <p>An optional data opener identifier.</p> <code>None</code> <p>Returns:</p> Type Description <code>JsonObjectSchema</code> <p>The schema for the parameters in open_params.</p> <p>Raises:</p> Type Description <code>DataStoreError</code> <p>If an error occurs.</p> Source code in <code>xcube/core/store/store.py</code> <pre><code>@abstractmethod\ndef get_open_data_params_schema(\n    self, data_id: str = None, opener_id: str = None\n) -&gt; JsonObjectSchema:\n    \"\"\"Get the schema for the parameters passed as *open_params* to\n    :meth:`open_data`.\n\n    If *data_id* is given, the returned schema will be tailored\n    to the constraints implied by the identified data resource.\n    Some openers might not support this, therefore *data_id* is optional,\n    and if it is omitted, the returned schema will be less restrictive.\n    If given, the method raises if *data_id* does not exist in this store.\n\n    If *opener_id* is given, the returned schema will be tailored to\n    the constraints implied by the identified opener. Some openers\n    might not support this, therefore *opener_id* is optional, and if\n    it is omitted, the returned schema will be less restrictive.\n\n    For maximum compatibility of stores, it is strongly encouraged to\n    apply the following conventions on parameter names, types,\n    and their interpretation.\n\n    Let P be the value of an optional, data constraining open parameter,\n    then it should be interpreted as follows:\n\n      * _if P is None_ means, parameter not given,\n        hence no constraint applies, hence no additional restrictions\n        on requested data.\n      * _if not P_ means, we exclude data that would be\n        included by default.\n      * _else_, the given constraint applies.\n\n    Given here are names, types, and descriptions of common,\n    constraining open parameters for gridded datasets.\n    Note, whether any of these is optional or mandatory depends\n    on the individual data store. A store may also\n    define other open parameters or support only a subset of the\n    following. Note all parameters may be optional,\n    the Python-types given here refer to _given_, non-Null parameters:\n\n      * ``variable_names: List[str]``: Included data variables.\n        Available coordinate variables will be auto-included for\n        any dimension of the data variables.\n      * ``bbox: Tuple[float, float, float, float]``: Spatial coverage\n        as xmin, ymin, xmax, ymax.\n      * ``crs: str``: Spatial CRS, e.g. \"EPSG:4326\" or OGC CRS URI.\n      * ``spatial_res: float``: Spatial resolution in\n        coordinates of the spatial CRS.\n      * ``time_range: Tuple[Optional[str], Optional[str]]``:\n        Time range interval in UTC date/time units using ISO format.\n        Start or end time may be missing which means everything until\n        available start or end time.\n      * ``time_period: str`: Pandas-compatible period/frequency\n        string, e.g. \"8D\", \"2W\".\n\n    E.g. applied to an optional `variable_names` parameter, this means\n\n      * `variable_names is None` - include all data variables\n      * `variable_names == []` - do not include data variables\n        (schema only)\n      * `variable_names == [\"&lt;var_1&gt;\", \"&lt;var_2&gt;\", ...]` only\n        include data variables named \"&lt;var_1&gt;\", \"&lt;var_2&gt;\", ...\n\n    Args:\n        data_id: An optional data identifier that is known to exist\n            in this data store.\n        opener_id: An optional data opener identifier.\n\n    Returns:\n        The schema for the parameters in *open_params*.\n\n    Raises:\n        DataStoreError: If an error occurs.\n    \"\"\"\n</code></pre>"},{"location":"api/#xcube.core.store.DataStore.open_data","title":"<code>open_data(data_id, opener_id=None, **open_params)</code>  <code>abstractmethod</code>","text":"<p>Open the data given by the data resource identifier data_id using the supplied open_params.</p> <p>If opener_id is given, the identified data opener will be used to open the data resource and open_params must comply with the schema of the opener's parameters. Note that some store implementations may not support using different openers or just support a single one.</p> <p>Implementations are advised to support an additional optional keyword argument <code>data_type: DataTypeLike = None</code>. If data_type is provided, the return value will be in the specified data type. If no data opener exists for the given data_type and format extracted from the data_id, the default data type alias 'dataset' will be used. Note that opener_id includes the data_type at its first position and will override the date_type argument.</p> <p>Raises if data_id does not exist in this store.</p> <p>Parameters:</p> Name Type Description Default <code>data_id</code> <code>str</code> <p>The data identifier that is known to exist in this data store.</p> required <code>opener_id</code> <code>str</code> <p>An optional data opener identifier.</p> <code>None</code> <code>**open_params</code> <p>Opener-specific parameters. Note that <code>data_type: DataTypeLike = None</code> may be assigned here.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>An in-memory representation of the data resources identified</p> <code>Any</code> <p>by data_id and open_params.</p> <p>Raises:</p> Type Description <code>DataStoreError</code> <p>If an error occurs.</p> Source code in <code>xcube/core/store/store.py</code> <pre><code>@abstractmethod\ndef open_data(\n    self,\n    data_id: str,\n    opener_id: str = None,\n    **open_params,\n) -&gt; Any:\n    \"\"\"Open the data given by the data resource identifier *data_id*\n    using the supplied *open_params*.\n\n    If *opener_id* is given, the identified data opener will be used\n    to open the data resource and *open_params* must comply with the\n    schema of the opener's parameters. Note that some store\n    implementations may not support using different openers or just\n    support a single one.\n\n    Implementations are advised to support an additional optional keyword\n    argument `data_type: DataTypeLike = None`.\n    If *data_type* is provided, the return value will be in the specified\n    data type. If no data opener exists for the given *data_type* and format\n    extracted from the *data_id*, the default data type alias 'dataset' will\n    be used. Note that *opener_id* includes the *data_type* at its first\n    position and will override the *date_type* argument.\n\n    Raises if *data_id* does not exist in this store.\n\n    Args:\n        data_id: The data identifier that is known to exist in this\n            data store.\n        opener_id: An optional data opener identifier.\n        **open_params: Opener-specific parameters. Note that\n            `data_type: DataTypeLike = None` may be assigned here.\n\n    Returns:\n        An in-memory representation of the data resources identified\n        by *data_id* and *open_params*.\n\n    Raises:\n        DataStoreError: If an error occurs.\n    \"\"\"\n</code></pre>"},{"location":"guide/","title":"User Guide","text":""},{"location":"guide/#accessing-eopf-sentinel-zarr-data-cubes-with-xcube","title":"Accessing EOPF Sentinel Zarr Data Cubes with xcube","text":"<p>The <code>\"eopf-zarr\"</code> xcube data store enables you to create analysis-ready data cubes  (ARDCs) from Sentinel Zarr sample products published by the  EOPF Sentinel Zarr Sample Service.</p> <p>This plugin provides convenient access to analysis-ready data from the Sentinel-1, Sentinel-2, and Sentinel-3 missions. Currently, only Sentinel-2 and Sentinel-3 products are supported.</p> <p>This guide walks you through:</p> <ol> <li>Set up a EOPF Data Store</li> <li>Selecting a Product</li> <li>Opening a Spatiotemporal Data Cube</li> <li>Inspecting, Visualizing, and Saving the Data Cube</li> </ol>"},{"location":"guide/#1-set-up-a-eopf-data-store","title":"1. Set up a EOPF Data Store","text":"<p>To instantiate the data store: <pre><code>from xcube.core.store import new_data_store\n\nstore = new_data_store(\"eopf-zarr\")\n</code></pre></p>"},{"location":"guide/#2-select-a-data-product","title":"2. Select a Data Product","text":"<p>Data products from Sentinel-1, -2, and -3 are accessed via the <code>data_id</code> parameter,  which corresponds to collection names in the EOPF STAC API.</p> <p>To list all available data IDs: <pre><code>store.list_data_ids()\n</code></pre> Each <code>data_id</code> is documented in the respective sections for the  supported missions below.</p>"},{"location":"guide/#3-open-a-spatiotemporal-data-cube","title":"3. Open a Spatiotemporal Data Cube","text":"<p>To open a cube (e.g., for Sentinel-2 Level-2A): <pre><code>ds = store.open_data(\n    data_id=\"sentinel-2-l2a\",\n    bbox=[9.7, 53.4, 10.3, 53.7],\n    time_range=[\"2025\u201105\u201101\", \"2025\u201105\u201107\"],\n    spatial_res=10 / 111320,  # here approx. 10m in degrees\n    crs=\"EPSG:4326\",\n)\n</code></pre> It uses the xarray-eopf backend  as a reading routine to open the EOPF Zarr  Samples. </p> <p>\ud83d\udca1 Note <code>open_data()</code> builds a Dask graph and returns a lazy <code>xarray.Dataset</code>. No actual data is loaded at this point. </p> <p>Required parameters:</p> <ul> <li><code>bbox</code>: Bounding box [\"west\", \"south\", \"est\", \"north\"] in CRS coordinates.</li> <li><code>time_range</code>: Temporal extent [\"YYYY-MM-DD\", \"YYYY-MM-DD\"].</li> <li><code>spatial_res</code>: Spatial resolution in meter of degree (depending on the CRS).</li> </ul> <p>These parameters control the STAC API query and define the output cube's spatial grid.</p>"},{"location":"guide/#optional-parameters","title":"Optional Parameters","text":"<ul> <li><code>crs</code>: Coordinate reference system, defaults to <code>\"EPSG:4326\"</code>.</li> <li><code>variables</code>: Variables to include in the dataset. Accepts a single name, a regex pattern, or an iterable of either.</li> <li><code>tile_size</code>: Spatial tile size of the returned dataset, given as <code>(width, height)</code>.</li> <li><code>query</code>: Additional filtering options for STAC Items by their properties. See the    STAC Query Extension for details.</li> <li><code>agg_methods</code>: Aggregation method(s) for downsampling spatial variables. Accepts:  </li> <li>A single method applied to all variables, or  </li> <li>A dictionary mapping variable names or dtypes to methods.</li> </ul> <p>Supported methods include: <code>\"center\"</code>, <code>\"count\"</code>, <code>\"first\"</code>, <code>\"last\"</code>, <code>\"max\"</code>, <code>\"mean\"</code>, <code>\"median\"</code>, <code>\"mode\"</code>, <code>\"min\"</code>, <code>\"prod\"</code>, <code>\"std\"</code>, <code>\"sum\"</code>, <code>\"var\"</code>.  </p> <p>Default: <code>\"center\"</code> for integer arrays, <code>\"mean\"</code> otherwise.   For details, see the xcube-resampling documentation.  </p> <ul> <li><code>interp_methods</code>: Interpolation method(s) for upsampling spatial variables. Accepts:  </li> <li>A single method applied to all variables, or  </li> <li>A dictionary mapping variable names or dtypes to methods.  </li> </ul> <p>Supported methods include:   - <code>0</code> \u2014 nearest neighbor (default for categorical / integer datasets)   - <code>1</code> \u2014 linear / bilinear (default for float datasets)   - <code>\"nearest\"</code> - alias for <code>0</code>    - <code>\"triangular\"</code> - linearly interpolate between 4 points using two triangles    - <code>\"bilinear\"</code> - alias for <code>1</code> </p> <p>For details, see the xcube-resampling documentation. </p>"},{"location":"guide/#4-inspect-visualize-and-save-the-data-cube","title":"4. Inspect, Visualize, and Save the Data Cube","text":"<p>You can visualize a time slice:</p> <pre><code>ds.b04.isel(time=0).plot()\n</code></pre> <p>\u26a0\ufe0f Warning This operation triggers data downloads and processing. For large regions, use with care.</p>"},{"location":"guide/#saving-the-data-cube","title":"Saving the Data Cube","text":"<p>Although the EOPF xcube plugin focuses on data access, it integrates seamlessly with  the broader xcube ecosystem for post-processing and storage.</p> <p>To persist the data cube, write it to a local file or S3-compatible object storage  using the <code>file</code> or <code>s3</code> xcube data store backends:</p> <p>Local Filesystem Data Store: <pre><code>storage = new_data_store(\"file\")\n</code></pre> S3 Data Store: <pre><code>storage = new_data_store(\n    \"s3\",\n    root=\"bucket-name\",\n    storage_options=dict(\n        anon=False,\n        key=\"your_s3_key\",\n        secret=\"your_s3_secret\",\n    ),\n)\n</code></pre> More info: Filesystem-based data stores.</p> <p>Then, write the cube:</p> <pre><code>storage.write_data(ds, \"path/to/file.zarr\")\n</code></pre>"},{"location":"guide/#visualize-in-xcube-viewer","title":"Visualize in xcube Viewer","text":"<p>Once saved as Zarr, you can use xcube Viewer, to visualize the cube:</p> <pre><code>from xcube.webapi.viewer import Viewer\n\nviewer = Viewer()\nds = storage.open_data(\"path/to/file.zarr\")\nviewer.add_dataset(ds)\nviewer.show()\n</code></pre> <p>To retrieve the temporary URL of the launched viewer as a web app:</p> <pre><code>viewer.info()\n</code></pre>"},{"location":"guide/#specific-parameters-for-supported-sentinel-missions","title":"Specific Parameters for supported Sentinel Missions","text":""},{"location":"guide/#sentinel-1","title":"\ud83d\udef0\ufe0f Sentinel-1","text":"<p>Support for Sentinel-1 will be added in an upcoming release.</p>"},{"location":"guide/#sentinel-2","title":"\ud83d\udef0\ufe0f Sentinel-2","text":"<p>Sentinel-2 provides multi-spectral imagery at different native resolutions:</p> <ul> <li>10m: b02, b03, b04, b08 </li> <li>20m: b05, b06, b07, b8a, b11, b12 </li> <li>60m: b01, b09, b10</li> </ul> <p>Sentinel-2 products are organized as STAC Items, each representing a single tile.  These tiles are stored in their native UTM CRS, which varies by geographic location.</p> <p>Data Identifiers</p> <p>The EOPF xcube data store supports two Sentinel-2 product types via the <code>data_id</code> argument:</p> Data ID Description <code>sentinel-2-l1c</code> Level\u20111C top\u2011of\u2011atmosphere (TOA) reflectance <code>sentinel-2-l2a</code> Level\u20112A atmospherically corrected surface reflectance <p>Supported Variables</p> <ul> <li>Surface reflectance bands: <code>b01</code>, <code>b02</code>, <code>b03</code>, <code>b04</code>, <code>b05</code>, <code>b06</code>, <code>b07</code>, <code>b08</code>, <code>b8a</code>, <code>b09</code>, <code>b11</code>, <code>b12</code></li> <li>Classification/Quality layers (L2A only): <code>cld</code>, <code>scl</code>, <code>snw</code></li> </ul> <p>Data Cube Generation Workflow</p> <ol> <li>STAC Query: A STAC API request returns relevant STAC Items (tiles) based on     spatial and temporal extent (<code>bbox</code> and <code>time_range</code> argument).</li> <li>Sorting: Items are ordered by solar acquisition time and Tile ID.</li> <li>Native Alignment: Within each UTM zone, tiles from the same solar day are     aligned in the native UMT without reprojection. Overlaps are resolved by selecting     the first non-NaN pixel value in item order.</li> <li>Cube Assembly: The method of cube creation depends on the user's request,   as summarized below:</li> </ol> Scenario Native Resolution Preservation Reprojected or Resampled Cube Condition Requested bounding box lies within a single UTM zone, native CRS is requested, and the spatial resolution matches the native resolution. Data spans multiple UTM zones, a different CRS is requested (e.g., EPSG:4326), or a custom spatial resolution is requested. Processing steps Only upsampling or downsampling is applied to align the differing resolutions of the spectral bands. Data cube is directly cropped using the requested bounding box, preserving original pixel values. Spatial extent may deviate slightly due to alignment with native pixel grid. A target grid mapping is computed from bounding box, spatial resolution, and CRS. Data from each UTM zone is reprojected/resampled to this grid. Overlaps resolved by first non-NaN pixel. <p>Users can specify any spatial resolution and coordinate reference system (CRS) when  opening data with <code>open_data</code>. As a result, spectral bands may be resampled \u2014 either  upsampled or downsampled \u2014 and reprojected to match the target grid. If reprojection  is needed at a lower resolution, the process first downsamples the data and  subsequently performs the reprojection. Upsampling and downsampling are controlled using the <code>agg_methods</code> and <code>interp_methods</code> parameters (see Optional Parameters).</p> <p>Furthermore, the parameter <code>crs</code> can be set to <code>\"native\"</code>, which allows the user to provide the bounding box in regular latitude/longitude coordinates while retrieving the data in its native UTM grid\u2014without triggering any reprojection. The following snippet shows an example:</p> <p>```python from xcube.core.store import new_data_store</p> <p>store = new_data_store(\"eopf-stac\") ds = store.open_data(     data_id=\"sentinel-2-l2a\",     bbox=[9.7, 53.4, 10.3, 53.7],     time_range=[\"2025-05-01\", \"2025-05-07\"],     spatial_res=10,     crs=\"native\",     variables=[\"b02\", \"b03\", \"b04\", \"scl\"], )</p> <p>Note that if the requested area spans multiple UTM zones, a <code>DataStoreError</code> will be raised.</p>"},{"location":"guide/#sentinel-3","title":"\ud83d\udef0\ufe0f Sentinel-3","text":"<p>Sentinel-3 has two instruments on board: </p> <p>\ud83c\udf0a OLCI \u2014 Ocean and Land Colour Instrument</p> <ul> <li>Purpose: Primarily designed for ocean and land surface monitoring.</li> <li>Spectral bands: 21 bands (400\u20131020 nm).</li> <li>Spatial resolution: 300 m.</li> <li>Swath width: ~1,270 km</li> </ul> <p>\ud83d\udd25 SLSTR \u2014 Sea and Land Surface Temperature Radiometer</p> <ul> <li>Purpose: Measures global sea and land surface temperatures with high accuracy.</li> <li>Spectral bands: 9 bands (visible to thermal infrared, 0.55\u201312 \u03bcm).</li> <li>Spatial resolution: 500 m (visible &amp; shortwave infrared bands) and 1 km    (thermal infrared bands).</li> <li>Swath width: ~1,400 km</li> </ul> <p>Sentinel-3 data products are distributed as STAC Items, where each item  corresponds to a single tile. The datasets are provided in their  native 2D irregular grid and typically require rectification for analysis-ready  applications. </p> <p>Data Identifiers</p> <p>The EOPF xcube data store so far supports three Sentinel-3 product types via the  <code>data_id</code> argument:</p> Data ID Description <code>sentinel-3-olci-l1-efr</code> Level-1 full-resolution top-of-atmosphere radiances from the OLCI <code>sentinel-3-olci-l2-lfr</code> Level-2 land and atmospheric geophysical parameters derived from OLCI <code>sentinel-3-slstr-l1-rbt</code> Level-1 radiances and brightness temperatures derived from SLSTR <code>sentinel-3-slstr-l2-lst</code> Level-2 land surface temperature products derived from SLSTR <p>Supported Variables</p> <ul> <li><code>sentinel-3-olci-l1-efr</code>: <code>oa01_radiance</code>, <code>oa02_radiance</code>, <code>oa03_radiance</code>, <code>oa04_radiance</code>, <code>oa05_radiance</code>,   <code>oa06_radiance</code>, <code>oa07_radiance</code>, <code>oa08_radiance</code>, <code>oa09_radiance</code>, <code>oa10_radiance</code>,   <code>oa11_radiance</code>, <code>oa12_radiance</code>, <code>oa13_radiance</code>, <code>oa14_radiance</code>, <code>oa15_radiance</code>,   <code>oa16_radiance</code>, <code>oa17_radiance</code>, <code>oa18_radiance</code>, <code>oa19_radiance</code>, <code>oa20_radiance</code>,   <code>oa21_radiance</code></li> <li><code>sentinel-3-olci-l2-lfr</code>: <code>gifapar</code>, <code>iwv</code>, <code>otci</code>, <code>rc681</code>, <code>rc865</code></li> <li><code>sentinel-3-slstr-l1-rbt</code>:   <code>s1_radiance_an</code>, <code>s2_radiance_an</code>, <code>s3_radiance_an</code>, <code>s4_radiance_an</code>,   <code>s5_radiance_an</code>, <code>s6_radiance_an</code>, <code>s1_radiance_ao</code>, <code>s2_radiance_ao</code>,   <code>s3_radiance_ao</code>, <code>s4_radiance_ao</code>, <code>s5_radiance_ao</code>, <code>s6_radiance_ao</code>,   <code>s4_radiance_bn</code>, <code>s5_radiance_bn</code>, <code>s6_radiance_bn</code>, <code>s4_radiance_bo</code>,   <code>s5_radiance_bo</code>, <code>s6_radiance_bo</code>, <code>f1_bt_fn</code>, <code>f1_bt_fo</code>, <code>f2_bt_in</code>,   <code>f2_bt_io</code>, <code>s7_bt_in</code>, <code>s8_bt_in</code>, <code>s9_bt_in</code>, <code>s7_bt_io</code>, <code>s8_bt_io</code>,   <code>s9_bt_io</code></li> <li><code>sentinel-3-slstr-l2-lst</code>: <code>lst</code></li> </ul> <p>Data Cube Generation Workflow</p> <p>The workflow for building 3D analysis-ready cubes from Sentinel-3 products involves  the following steps:</p> <ol> <li>Query tiles using the EOPF Zarr Sample Service STAC API for a given time range and     spatial extent.</li> <li>Group items by solar day.</li> <li>Rectify data from the native 2D irregular grid to a regular grid using     xcube-resampling.</li> <li>Mosaic adjacent tiles into seamless daily scenes.</li> <li>Stack the daily mosaics along the temporal axis to form 3D data cubes     for each variable (e.g., spectral bands).</li> </ol> <p>\u26a0\ufe0f Important considerations:  </p> <ul> <li>Rectification (irregular \u2192 regular grid) is computationally expensive and may    slow down cube generation.  </li> <li>Users can specify any spatial resolution and coordinate reference system (CRS) when    opening data with <code>open_data</code>. During rectification, spectral bands are internally    reprojected to the target grid. If a lower-resolution grid is requested,    downsampling is applied prior to rectification.  </li> <li>Resampling behavior is controlled via the <code>agg_methods</code>    (downsampling) and <code>interp_methods</code>    (upsampling/interpolation) parameters.  </li> </ul>"},{"location":"start/","title":"Getting Started","text":"<p>The <code>xcube-eopf</code> package can be installed into an existing Python environment using</p> <pre><code>pip install xcube-eopf\n</code></pre> <p>or</p> <pre><code>conda install -c conda-forge xcube-eopf\n</code></pre> <p>After installation, you are ready to go and use the <code>\"eopf-zarr\"</code> argument to initiate a xcube EOPF data store.</p> <pre><code>from xcube.core.store import new_data_store\n\nstore = new_data_store(\"eopf-stac\")\nds = store.open_data(\n    data_id=\"sentinel-2-l2a\",\n    bbox=[9.7, 53.4, 10.3, 53.7],\n    time_range=[\"2025-05-01\", \"2025-05-07\"],\n    spatial_res=10 / 111320,  # meters converted to degrees (approx.)\n    crs=\"EPSG:4326\",\n    variables=[\"b02\", \"b03\", \"b04\", \"scl\"],\n)\n</code></pre>"}]}